{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API REQUESTS AND JSON READ FUNCTIONS\n",
    "-Inputs = Coins, Timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "import My_API_Wraps\n",
    "import My_FE_Wraps\n",
    "import My_FS_Wraps\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import xgboost\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from xgboost import dask as dxgb\n",
    "import os\n",
    "import cupy as cp\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU optimized cluster for my CPU \n",
    "cluster = LocalCluster(\n",
    "            n_workers=4,\n",
    "            threads_per_worker=5,\n",
    "            processes=True,\n",
    "            # memory_limit='12GB',\n",
    "            dashboard_address=':8787',\n",
    "            resources = {'GPU':1}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work in progress-GPU optimized cluster with CUDA\n",
    "# Notes:Windows doesn't support dask.distributed.LocalCudaCluster\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "cp.cuda.set_allocator(cp.cuda.MemoryPool().malloc)\n",
    "\n",
    "cluster = LocalCluster(\n",
    "    n_workers=1,              # Single worker for GPU\n",
    "    processes=False,          # Thread-based\n",
    "    threads_per_worker=20,    # Maximum threads for i7-13700H\n",
    "    memory_limit='22GB',      # 75% of system RAM\n",
    "    dashboard_address=':8788',\n",
    "    resources={'GPU': 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key function parameters and setting up dask client\n",
    "# Notes: Use client for real time analytics to optimise number of partitions, cluster, and dataflow\n",
    "# If timeframe =1, periods are 5minute intervals, else hourly\n",
    "client = Client(cluster)\n",
    "periods= 1\n",
    "timeframe= 7\n",
    "top_coins= 5\n",
    "api_key= \"CG-r57ENE22zzPUiLmjnyFK7YHw\"\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"x-cg-demo-api-key\": api_key\n",
    "    }\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Use to close client and cluster when done with daskcomputations\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m      3\u001b[0m cluster\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "# Use to close client and cluster when done with daskcomputations\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Example 1- Multiprocessing Distributor\n",
    "df_pandas= My_API_Wraps.CoinGecko_HSPD_Pandas(timeframe, top_coins, periods, api_key)\n",
    "EF_pandas= My_FE_Wraps.EF_Pandas_MultiprocessingDistributor(df_pandas, ParameterComplexity=0)\n",
    "SF_pandas= My_FS_Wraps.SF_Pandas_Simple(EF_pandas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Example 2- Pandas with Dask Distributor\n",
    "df_pandas= My_API_Wraps.CoinGecko_HSPD_Pandas(timeframe, top_coins, periods, api_key)\n",
    "EF_pandas2= My_FE_Wraps.EF_Pandas_DaskDistributor(df_pandas, cluster.scheduler_address, ParameterComplexity=0)\n",
    "SF_pandas= My_FS_Wraps.SF_Pandas_Simple(EF_pandas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask Example- Data Collection, Feature Extraction & Selection\n",
    "# Notes: can also use .persist on functions rather than clients\n",
    "# (Recommended) Keep ef splits to 1 as the next step feature selection occurs on per-partition basis. \n",
    "# (Not Recommended) Otherwise call X.columns.tolist() for features selected over multiple partitions \n",
    "raw_data= client.submit(My_API_Wraps.CoinGecko_HSPD_Dask, timeframe=timeframe, top_coins=top_coins, periods=periods, api_key=api_key, splits=timeframe*3)\n",
    "ef= client.submit(My_FE_Wraps.EF_Dask, raw_data, ParameterComplexity=1, splits=1, LR=True)\n",
    "y = client.persist(ef.result()['y_future'].dropna().repartition(npartitions=10))\n",
    "sf= client.submit(My_FS_Wraps.SF_Dask_v1, ef, p_value=0.05)\n",
    "X= client.persist(sf.result().repartition(npartitions=10))\n",
    "X.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask Example- Test-Train Split by Partitions and DMatrix construction\n",
    "X_train= X.partitions[0:7].persist()\n",
    "y_train= y.partitions[0:7].persist()\n",
    "X_test= X.partitions[7:9].persist()\n",
    "y_test= y.partitions[7:9].persist()\n",
    "dtrain = dxgb.DaskDMatrix(client, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask Example- Hyperparmater optimization with Dask and Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"verbosity\": 0,\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"eval_metric\": 'mae',\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 100.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 100.0, log=True),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-8, 100, log=True),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-8, 1.0, log=True),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "    }\n",
    "    try:\n",
    "        output = dxgb.train(\n",
    "            client,\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=20,\n",
    "            evals=[(dtrain, \"train\")]\n",
    "        )\n",
    "        return output[\"history\"][\"train\"][\"mae\"][-1]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in training: {str(e)}\")\n",
    "        return float('inf')\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=40, n_jobs=-1, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask Example- Dask optuna final model results\n",
    "final_model= dxgb.train(client, study.best_params, dtrain, num_boost_round=300, evals=[(dtrain, \"train\")])\n",
    "dtest = dd.from_pandas(X_test.compute())\n",
    "predictions = dxgb.predict(client, final_model, dtest)\n",
    "r2 = r2_score(y_test.compute(), predictions)\n",
    "std = y_test.std().compute()\n",
    "score = study.best_value\n",
    "Thresh_var = score/std \n",
    "print('Standard_Dev: '+ f'{std}')\n",
    "print(f\"Best parameters: {study.best_params}\")\n",
    "print(f\"Best RMSE: {study.best_value}\")\n",
    "print(f\"R2 Score: {r2}\")\n",
    "print(f'score/std: {Thresh_var}')\n",
    "viz= pd.DataFrame(columns=[\"Predicted\", \"Actual\"], index=y_test.index.compute())\n",
    "viz['Predicted'] = predictions\n",
    "viz['Actual'] = y_test.compute().values\n",
    "viz.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas- GPU Accelerated XGBoost Using Random Gridsearch for Hyperparameter Tuning \n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model_parameters={\n",
    "'num_parallel_tree': [1,3,5,10,20],\n",
    "'learning_rate': [0.01,0.01,0.05,0.1,0.5,1],\n",
    "'max_depth': [3,6,12,24,48],\n",
    "'gamma':[0,0.01,0.05,0.1,0.5,1,5,25],\n",
    "'min_child_weight':[0.5,1,3,5],\n",
    "'subsample':[1,0.05, 0.1,0.25,0.5],\n",
    "'sampling_method':['uniform', 'gradient_based'],\n",
    "'colsample_bytree':[1, 0.1, 0.5, 1],\n",
    "'grow_policy':['depthwise', 'lossguide']\n",
    "}\n",
    "basemodel= xgb.XGBRegressor(eval_metric='mae', early_stopping_rounds=25, \n",
    "                        device= 'cuda',tree_method='hist', n_jobs=1, verbose=0)\n",
    "rgs= RandomizedSearchCV(estimator=basemodel, verbose= 25, n_jobs=1,\n",
    "                        cv=tscv,param_distributions=model_parameters, n_iter=20, refit=True).fit(X_train, y_train, eval_set=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas- Final Model Results\n",
    "chosen_model= xgb.XGBRegressor(eval_metric='mae', early_stopping_rounds= 25, device= 'cuda',tree_method='hist',**rgs.best_params_, n_jobs=1)\n",
    "chosen_model.fit(X_train_cp, y_train_cp, eval_set= [(X_test_cp, y_test_cp)])\n",
    "preds=chosen_model.predict(X_test)\n",
    "estimated_performance = r2_score(y_test, preds)\n",
    "predictions = pd.DataFrame(preds, columns= ['predicted_testtrain'], index= X_test.index.shift(freq=f'{periods}min'))\n",
    "predictions['realised']= y_test.values\n",
    "predictions= predictions* 100\n",
    "score = chosen_model.best_score*100\n",
    "std = features['y_future'].std()*100\n",
    "print('Mean_absolute error: '+ f'{score}')\n",
    "print('Standard_Dev: '+ f'{std}')\n",
    "print(f'R2: {estimated_performance}')\n",
    "feature_imp = pd.DataFrame(data = chosen_model.feature_importances_, index= chosen_model.feature_names_in_, columns=['f_imp']).sort_values(ascending=False, by='f_imp')\n",
    "Thresh_var = score/std \n",
    "print(f'score/std: {Thresh_var}')\n",
    "display(feature_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pandas- Final model for deployment and plot of predicted, realised, and untested future predictions\n",
    "final_model = xgb.XGBRegressor(device= 'cuda',tree_method='hist',**rgs.best_params_)\n",
    "final_model.fit(X, y)\n",
    "deployment_preds= features.drop('y_future', axis=1)[X.columns].tail(shift)\n",
    "dep_preds=final_model.predict(deployment_preds)\n",
    "dep_preds= pd.Series(dep_preds, index=deployment_preds.index.shift(freq=freq), name='untested_predictions')\n",
    "predictions.join(dep_preds, how='outer').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work in Progress-  Using PancakeSwap API to set up trading bot \n",
    "# import requests\n",
    "# import json\n",
    "\n",
    "# # Replace with your PancakeSwap API endpoint and API key\n",
    "API_ENDPOINT = 'https://api.pancakeswap.info/api/v2/orders'\n",
    "API_KEY = 'your_api_key_here'\n",
    "\n",
    "def place_order(order_type, amount, leverage, price=None):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {API_KEY}'\n",
    "    }\n",
    "    order_data = {\n",
    "        'type': order_type,  # 'buy' or 'sell'\n",
    "        'amount': amount,    # Amount of ETH to trade\n",
    "        'leverage': leverage # Leverage to use\n",
    "    }\n",
    "    if price:\n",
    "        order_data['price'] = price  # Set price for limit orders\n",
    "    else:\n",
    "        order_data['price'] = 'market'  # Set price to market for market orders\n",
    "\n",
    "    response = requests.post(API_ENDPOINT, headers=headers, data=json.dumps(order_data))\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Order placed successfully: {response.json()}\")\n",
    "    else:\n",
    "        print(f\"Failed to place order: {response.status_code}, {response.text}\")\n",
    "\n",
    "# Example usage\n",
    "place_order('buy', 0.1, 10)  # Place a market buy order for 0.1 ETH with 10x leverage\n",
    "place_order('sell', 0.1, 10, 3600.10)  # Place a limit sell order for 0.1 ETH at $3600.10 with 10x leverage\n",
    "# In this example, if you don't provide a price, the order will be placed at the market price. If you provide a price, it will be treated as a limit order. This way, you can easily switch between market and limit orders based on your trading strategy. Happy trading!\n",
    "\n",
    "\n",
    "def execute_trade(prediction): \n",
    "    if prediction >= 0.02: \n",
    "        place_order('buy', 0.1, 10)  # Place a buy order for 0.1 ETH at $3600.10 with 10x leverage\n",
    "    elif prediction <= -0.02:\n",
    "        place_order('sell', 0.1, 10)\n",
    "\n",
    "# # Main trading loop\n",
    "# while True:\n",
    "#     market_data = fetch_market_data()\n",
    "#     prediction = make_prediction(market_data)\n",
    "#     execute_trade(prediction)\n",
    "#     # Sleep for 30 minutes before making the next prediction\n",
    "#     time.sleep(1800)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
