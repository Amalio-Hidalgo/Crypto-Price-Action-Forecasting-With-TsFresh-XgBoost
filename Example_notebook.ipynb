{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryptocurrency Price Action Analysis\n",
    "\n",
    "This notebook demonstrates different approaches to cryptocurrency price prediction:\n",
    "1. Pandas with local processing\n",
    "2. Pandas with Dask distribution\n",
    "3. Full Dask implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "import My_API_Wraps\n",
    "import My_FE_Wraps\n",
    "import My_FS_Wraps\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import xgboost\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from xgboost import dask as dxgb\n",
    "import os\n",
    "import cupy as cp\n",
    "import optuna\n",
    "import My_ML_Wraps\n",
    "import warnings\n",
    "import coiled\n",
    "import My_Utilities\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coiled Cluster Connecting to my AWS Account\n",
    "cluster = coiled.Cluster(\n",
    "    # n_workers= 3, # 1 to 3 workers\n",
    "    # worker_memory=\"32GiB\",\n",
    "    region=\"eu-west-3\",\n",
    "    shutdown_on_close=True\n",
    ")\n",
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU optimized cluster for my CPU \n",
    "cluster = LocalCluster(\n",
    "            n_workers=4,\n",
    "            threads_per_worker=5,\n",
    "            processes=True,\n",
    "            dashboard_address=':8787',\n",
    "            resources = {'GPU':2}\n",
    "        )\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work in progress-GPU optimized cluster with CUDA\n",
    "# Notes:Windows doesn't support dask.distributed.LocalCudaCluster\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "cp.cuda.set_allocator(cp.cuda.MemoryPool().malloc)\n",
    "\n",
    "cluster = LocalCluster(\n",
    "    n_workers=1,              # Single worker for GPU\n",
    "    processes=False,          # Thread-based\n",
    "    threads_per_worker=32,    # Maximum threads for i7-13700H\n",
    "    # memory_limit='22GB',      # 75% of system RAM\n",
    "    dashboard_address=':8788',\n",
    "    resources={'GPU': 1}\n",
    ")\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use to close client and cluster when done with Dask computations\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key function parameters \n",
    "# Notes: \n",
    "# If timeframe=1, periods are 5minute intervals, else hourly\n",
    "# Max timeframe=89, after lose hourly granularity\n",
    "# For timeframe=1, setting periods higher than 3 will result in inusfficient partition sizes for feature extraction\n",
    "periods= 1\n",
    "timeframe= 1\n",
    "top_coins= 2\n",
    "api_key= \"CG-r57ENE22zzPUiLmjnyFK7YHw\"\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"x-cg-demo-api-key\": api_key\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask Example- Dask Raw Data, Feature Extraction & Selection, and Train-Test Split\n",
    "# Notes:\n",
    "# Make sure significant features were found by computing some data from X_train/X_test; \n",
    "#   - this code doesnt return an error message like Pandas version\n",
    "future0= client.submit(My_API_Wraps.CoinGecko_HSPD_Dask, timeframe=timeframe, top_coins=top_coins, periods=periods, api_key=api_key)\n",
    "future1= client.submit(My_FE_Wraps.EF_Dask, future0, ParameterComplexity=0, LR=False, Vol=False)\n",
    "future2 = client.submit(My_FS_Wraps.SF_Dask, future1, p_value=0.10)\n",
    "X, y= future2.result()\n",
    "X = X.repartition(npartitions=5)\n",
    "y= y.repartition(npartitions=5)\n",
    "X_train= X.partitions[0:-1]\n",
    "X_test= X.partitions[-1]\n",
    "y_train= y.partitions[0:-1]\n",
    "y_test= y.partitions[-1]\n",
    "X_train, X_test, y_train, y_test = client.persist([X_train, X_test, y_train, y_test])\n",
    "dtrain= dxgb.DaskDMatrix(client, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask Example:Optuna Hyperparameter Optimization and Model Training\n",
    "study= My_ML_Wraps.Optuna_XGB_Dask(client, dtrain,  n_trials=50, n_rounds=100, eval_metric= 'mape', tree_method='hist', early_stopping_rounds=20)\n",
    "final_model= dxgb.train(client, study.best_params, dtrain, num_boost_round=100, evals=[(dtrain, \"train\")])\n",
    "model_features = final_model['booster'].feature_names\n",
    "dtest= dxgb.DaskDMatrix(client, X_test[model_features])\n",
    "predictions = dxgb.predict(client, final_model, dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask Example:Final Model Results, Evaluation, & Visualisation\n",
    "y_test= y_test.compute()\n",
    "predictions= pd.Series(predictions.compute(), index=y_test.index)\n",
    "r2 = r2_score(y_true=y_test, y_pred=predictions)\n",
    "std = y_test.std()\n",
    "score = study.best_value\n",
    "Thresh_var = score/std \n",
    "print('Standard_Dev: '+ f'{std}')\n",
    "print(f\"Best parameters: {study.best_params}\")\n",
    "print(f\"Best MAPE: {study.best_value}\")\n",
    "print(f\"R2 Score: {r2}\")\n",
    "# print(f'Score/Std: {Thresh_var}')\n",
    "viz = pd.concat([y_test, predictions],axis=1)\n",
    "viz.columns = ['Actual', 'Predicted']\n",
    "viz.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Example 1-Pandas with Multiprocessing Distributor: \n",
    "# Raw Data, Feature Extraction & Selection, Visualization Results\n",
    "df_pandas= My_API_Wraps.CoinGecko_HSPD_Pandas(timeframe, top_coins, periods, api_key)\n",
    "EF_pandas= My_FE_Wraps.EF_Pandas_MultiprocessingDistributor(df_pandas, ParameterComplexity=0)\n",
    "SF_pandas= My_FS_Wraps.SF_Pandas_v1(EF_pandas)\n",
    "SF_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Example 1-Pandas with Multiprocessing Distributor: \n",
    "# Train-Test Split, XGBoost Hyperparameter Optimization & Model Training, Evaluation, & Visualisation\n",
    "X= SF_pandas.drop('y_future', axis=1)\n",
    "y= SF_pandas['y_future']\n",
    "X_train = X.iloc[:int(-0.2*len(SF_pandas))]\n",
    "y_train = y.iloc[:int(-0.2*len(SF_pandas))]\n",
    "X_test = X.iloc[int(-0.2*len(SF_pandas)):]\n",
    "y_test = y.iloc[int(-0.2*len(SF_pandas)):]\n",
    "RGS= My_ML_Wraps.RGS_XGB_Pandas(X_train, X_test, y_train, y_test, parameter_grid=None, number_cvs=5)\n",
    "final_model= RGS.best_estimator_\n",
    "final_model= final_model.fit(X_train, y_train, eval_set=[(X_train, y_train)])\n",
    "preds=final_model.predict(X_test)\n",
    "r2= final_model.score(X_test, y_test)\n",
    "predictions = pd.DataFrame(preds, columns= ['predicted'], index= X_test.index)\n",
    "predictions['realised']= y_test.values\n",
    "predictions.plot(figsize=(20,10))\n",
    "score = RGS.best_score_\n",
    "std = y_test.std()\n",
    "print('RGS Best Score: '+ f'{score}')\n",
    "print('Y_test Standard_Dev: '+ f'{std}')\n",
    "print(f'R2: {r2}')\n",
    "feature_imp = pd.DataFrame(data = final_model.feature_importances_, index= final_model.feature_names_in_, columns=['f_imp']).sort_values(ascending=False, by='f_imp')\n",
    "Thresh_var = score/std \n",
    "print(f'score/std: {Thresh_var}')\n",
    "display(feature_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Example 2-Pandas with Dask Distributor: \n",
    "# Raw Data, Feature Extraction & Selection, Visualization Results\n",
    "df_pandas2= My_API_Wraps.CoinGecko_HSPD_Pandas(timeframe, top_coins, periods, api_key)\n",
    "EF_pandas2= My_FE_Wraps.EF_Pandas_DaskDistributor(df_pandas2, cluster.scheduler_address, ParameterComplexity=0)\n",
    "SF_pandas2= My_FS_Wraps.SF_Pandas_v1(EF_pandas2)\n",
    "SF_pandas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Example 2-Pandas with Dask Distributor: \n",
    "# Train-Test Split, XGBoost Hyperparameter Optimization & Model Training, Evaluation, & Visualisation\n",
    "X= SF_pandas2.drop('y_future', axis=1)\n",
    "y= SF_pandas2['y_future']\n",
    "X_train = X.iloc[:int(-0.2*len(SF_pandas2))]\n",
    "y_train = y.iloc[:int(-0.2*len(SF_pandas2))]\n",
    "X_test = X.iloc[int(-0.2*len(SF_pandas2)):]\n",
    "y_test = y.iloc[int(-0.2*len(SF_pandas2)):]\n",
    "RGS= My_ML_Wraps.RGS_XGB_Pandas(X_train, X_test, y_train, y_test, parameter_grid=None, number_cvs=5)\n",
    "final_model= RGS.best_estimator_\n",
    "final_model= final_model.fit(X_train, y_train, eval_set=[(X_train, y_train)])\n",
    "preds=final_model.predict(X_test)\n",
    "r2= final_model.score(X_test, y_test)\n",
    "predictions = pd.DataFrame(preds, columns= ['predicted'], index= X_test.index)\n",
    "predictions['realised']= y_test.values\n",
    "predictions.plot(figsize=(20,10))\n",
    "score = RGS.best_score_\n",
    "std = y_test.std()\n",
    "print('RGS Best Score: '+ f'{score}')\n",
    "print('Y_test Standard_Dev: '+ f'{std}')\n",
    "print(f'R2: {r2}')\n",
    "feature_imp = pd.DataFrame(data = final_model.feature_importances_, index= final_model.feature_names_in_, columns=['f_imp']).sort_values(ascending=False, by='f_imp')\n",
    "Thresh_var = score/std \n",
    "print(f'score/std: {Thresh_var}')\n",
    "display(feature_imp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
